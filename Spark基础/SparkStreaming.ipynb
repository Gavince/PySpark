{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark之SparkStreaming基本操作\n",
    "## 前言\n",
    "流数据具有如下特征：  \n",
    "•数据快速持续到达，潜在大小也许是无穷无尽的•数据来源众多，格式复杂  \n",
    "•数据量大，但是不十分关注存储，一旦经过处理，要么被丢弃，要么被归档存储  \n",
    "•注重数据的整体价值，不过分关注个别数据  \n",
    "•数据顺序颠倒，或者不完整，系统无法控制将要处理的新到达的数据元素的顺序  \n",
    "\n",
    "流计算(数据的价值随着时间的流式而降低)：  \n",
    "实时获取来自不同数据源的海量数据，经过实时分析处理，获得有价值的信息  \n",
    "\n",
    "流计算处理流程(强调实时性)：  \n",
    "数据实时采集--->数据实时计算--->实时查询服务  \n",
    "- 数据实时采集：数据实时采集阶段通常采集多个数据源的海量数据，需要保证实时性、低延迟与稳定可靠\n",
    "- 数据实时计算：数据实时计算阶段对采集的数据进行实时的分析和计算，并反馈实时结果\n",
    "- 实时查询服务：经由流计算框架得出的结果可供用户进行实时查询、展示或储存  \n",
    "\n",
    "流处理系统与传统的数据处理系统有如下不同：\n",
    "- 流处理系统处理的是实时的数据，而传统的数据处理系统处理的是预先存储好的静态数据\n",
    "- 用户通过流处理系统获取的是实时结果，而通过传统的数据处理系统，获取的是过去某一时刻的结果\n",
    "- 流处理系统无需用户主动发出查询，实时查询服务可以主动将实时结果推送给用户  \n",
    "\n",
    "SparkStreaming操作：  \n",
    "Spark Streaming可整合多种输入数据源，如Kafka、Flume、HDFS，甚至是普通的TCP套接字。经处理后的数据可存储至文件系统、数据库，或显示在仪表盘里。  \n",
    "![](./imgs/ssc.png)  \n",
    "Spark Streaming的基本原理:  \n",
    "是将实时输入数据流以时间片（秒级）为单位进行拆分，然后经Spark引擎以类似批处理的方式处理每个时间片数据（<font color = \"red\">伪实时</font>）  \n",
    "![](./imgs/ssc_pro.png)  \n",
    "Spark Streaming最主要的抽象:  \n",
    "DStream（Discretized Stream，离散化数据流），表示连续不断的数据流。在内部实现上，Spark Streaming的输入数据按照时间片（如1秒）分成一段一段，每一段数据转换为Spark中的RDD，这些分段就是Dstream，并且对DStream的操作都最终转变为对相应的RDD的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本操作\n",
    "编写Spark Streaming程序的基本步骤是：  \n",
    "1.通过创建输入DStream来定义输入源  \n",
    "2.通过对DStream应用转换操作和输出操作来定义流计算  \n",
    "3.用streamingContext.start()来开始接收数据和处理流程  \n",
    "4.通过streamingContext.awaitTermination()方法来等待处理结束（手动结束或因为错误而结束）  \n",
    "5.可以通过streamingContext.stop()来手动结束流计算进程  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD队列流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "import time\n",
    "\n",
    "# 环境配置\n",
    "conf = SparkConf().setAppName(\"RDD Queue\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting rdd_queue.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile rdd_queue.py\n",
    "import findspark  \n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "import time\n",
    "\n",
    "# 环境配置\n",
    "conf = SparkConf().setAppName(\"RDD Queue\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)\n",
    "ssc = StreamingContext(sc, 10)\n",
    "\n",
    "# 创建RDD数据流\n",
    "rddQueue = []\n",
    "for i in range(5):\n",
    "    rddQueue += [ssc.sparkContext.parallelize([j for j in range(i)], 10)]\n",
    "    time.sleep(1)\n",
    "    \n",
    "inputStream = ssc.queueStream(rddQueue)\n",
    "reduceedStream = inputStream.map(lambda x: (x%10, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "reduceedStream.pprint()\n",
    "ssc.start()\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "-------------------------------------------                                     \n",
      "Time: 2021-05-09 11:18:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-09 11:18:10\n",
      "-------------------------------------------\n",
      "(0, 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-09 11:18:20\n",
      "-------------------------------------------\n",
      "(0, 1)\n",
      "(1, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python rdd_queue.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词频统计(无状态操作)\n",
    "输出结果显示:  \n",
    "![](./imgs/ssc_out.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sscrun.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile sscrun.py\n",
    "import findspark  \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# 创建环境\n",
    "sc = SparkContext(master=\"local[2]\", appName=\"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 10)  # 每隔一秒钟监听一次数据\n",
    "\n",
    "# 监听端口数据\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "#　拆分单词\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "# 统计词频\n",
    "pairs = words.map(lambda word:(word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y:x+y)\n",
    "# 打印数据\n",
    "wordCounts.pprint()\n",
    "\n",
    "# 开启流式处理\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "-------------------------------------------                                     \n",
      "Time: 2021-05-09 10:47:40\n",
      "-------------------------------------------\n",
      "('1', 2)\n",
      "\n",
      "-------------------------------------------                                     \n",
      "Time: 2021-05-09 10:47:50\n",
      "-------------------------------------------\n",
      "('1', 18)\n",
      "\n",
      "^Ctage 0:>                                                          (0 + 1) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"./sscrun.py\", line 22, in <module>\n",
      "    ssc.awaitTermination()\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/context.py\", line 199, in awaitTermination\n",
      "    self._jssc.awaitTermination()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1303, in __call__\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1033, in send_command\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1200, in send_command\n",
      "  File \"/home/gavin/anaconda3/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 285, in signal_handler\n",
      "    raise KeyboardInterrupt()\n",
      "KeyboardInterrupt\n",
      "21/05/09 10:47:57 ERROR ReceiverTracker: Receiver has been stopped. Try to restart it.\n",
      "org.apache.spark.SparkException: Job 0 cancelled as part of cancellation of all jobs\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2149)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$doCancelAllJobs$2(DAGScheduler.scala:971)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:970)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2405)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "21/05/09 10:47:57 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n"
     ]
    }
   ],
   "source": [
    "!python ./sscrun.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词频统计(有状态操作)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting calcultor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile calcultor.py\n",
    "import findspark  \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# 创建环境\n",
    "sc = SparkContext(master=\"local[2]\", appName=\"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 3)  # 每隔一秒钟监听一次数据\n",
    "\n",
    "# 设置检测点\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "# 状态更新函数\n",
    "def updatefun(new_values,last_sum):\n",
    "    \n",
    "    # 向前转态加上当前状态的key状态的value值\n",
    "    return sum(new_values) + (last_sum or 0)\n",
    "\n",
    "# 监听端口数据\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "#　拆分单词\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "#  统计词频\n",
    "pairs = words.map(lambda word:(word, 1))\n",
    "wordCounts = pairs.updateStateByKey(updateFunc=updatefun)\n",
    "wordCounts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Traceback (most recent call last):\n",
      "  File \"calcultor.py\", line 11, in <module>\n",
      "    ssc.checkpoint(\"checkpoint\")\n",
      "  File \"/usr/local/spark/python/pyspark/streaming/context.py\", line 260, in checkpoint\n",
      "    self._jssc.checkpoint(directory)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1304, in __call__\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o24.checkpoint.\n",
      ": java.net.ConnectException: Call From gavin-X550JX/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:837)\n",
      "\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:757)\n",
      "\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1566)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1508)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1405)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:234)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:119)\n",
      "\tat com.sun.proxy.$Proxy17.mkdirs(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:673)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n",
      "\tat com.sun.proxy.$Proxy18.mkdirs(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2490)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2466)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1467)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1464)\n",
      "\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1481)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1456)\n",
      "\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2355)\n",
      "\tat org.apache.spark.streaming.StreamingContext.checkpoint(StreamingContext.scala:239)\n",
      "\tat org.apache.spark.streaming.api.java.JavaStreamingContext.checkpoint(JavaStreamingContext.scala:509)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:715)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:699)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:812)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:413)\n",
      "\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1636)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1452)\n",
      "\t... 36 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python calcultor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top.py\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "def get_countryname(line):\n",
    "    country_name = line.strip()\n",
    "\n",
    "    if country_name == 'usa':\n",
    "        output = 'USA'\n",
    "    elif country_name == 'ind':\n",
    "        output = 'India'\n",
    "    elif country_name == 'aus':\n",
    "        output = 'Australia'\n",
    "    else:\n",
    "        output = 'Unknown'\n",
    "\n",
    "    return (output, 1)\n",
    "\n",
    "# 设置参数\n",
    "batch_interval = 1\n",
    "window_length = 6*batch_interval\n",
    "frquency = 3*batch_interval\n",
    "\n",
    "sc =  sc = SparkContext(master=\"local[2]\", appName=\"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "# 监听端口数据\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "addFunc = lambda x, y: x+y\n",
    "invAddFunc = lambda x, y: x-y\n",
    "word_counts = lines.map(get_countryname).reduceByKeyAndWindow(addFunc, invAddFunc, window_length, frquency)\n",
    "word_counts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
