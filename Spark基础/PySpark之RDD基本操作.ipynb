{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark之RDD基本操作\n",
    "Spark是基于内存的计算引擎，它的计算速度非常快。但是仅仅只涉及到数据的计算，并没有涉及到数据的存储，但是，spark的缺点是：吃内存，不太稳定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总体而言，Spark采用RDD以后能够实现高效计算的主要原因如下：  \n",
    "（1）高效的容错性。现有的分布式共享内存、键值存储、内存数据库等，为了实现容错，必须在集群节点之间进行数据复制或者记录日志，也就是在节点之间会发生大量的数据传输，这对于数据密集型应用而言会带来很大的开销。在RDD的设计中，数据只读，不可修改，如果需要修改数据，必须从父RDD转换到子RDD，由此在不同RDD之间建立了血缘关系。<font color=\"blue\">所以，RDD是一种天生具有容错机制的特殊集合，不需要通过数据冗余的方式（比如检查点）实现容错，而只需通过RDD父子依赖（血缘）关系重新计算得到丢失的分区来实现容错，无需回滚整个系统，这样就避免了数据复制的高开销，而且重算过程可以在不同节点之间并行进行，实现了高效的容错。</font>此外，RDD提供的转换操作都是一些粗粒度的操作（比如map、filter和join），RDD依赖关系只需要记录这种粗粒度的转换操作，而不需要记录具体的数据和各种细粒度操作的日志（比如对哪个数据项进行了修改），这就大大降低了数据密集型应用中的容错开销；  \n",
    "（2）中间结果持久化到内存。数据在内存中的多个RDD操作之间进行传递，不需要“落地”到磁盘上，避免了不必要的读写磁盘开销；  \n",
    "（3）存放的数据可以是Java对象，避免了不必要的对象序列化和反序列化开销。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入门(词频统计)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个程序的执行过程如下：\n",
    "*  创建这个Spark程序的执行上下文，即创建SparkContext对象；\n",
    "*  从外部数据源或HDFS文件中读取数据创建words对象；\n",
    "*  构建起words和RDD之间的依赖关系，形成DAG图，这时候并没有发生真正的计算，只是记录转换的轨迹；\n",
    "*  执行到,collect()是一个行动类型的操作，触发真正的计算，开始实际执行从words到RDD的转换操作，并把结果持久化到内存中，最后计算出RDD中包含的元素个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkContext作为Spark的程序入口\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.context import SparkContext, SparkConf\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.191.4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "# 输出Spark基本信息\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**textFile的使用**:  \n",
    "Spark采用textFile()方法来从文件系统中加载数据创建RDD,该方法把文件的URI作为参数，这个URI可以是：  \n",
    "•本地文件系统的地址  \n",
    "•或者是分布式文件系统HDFS的地址  \n",
    "•或者是Amazon S3的地址等等  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file:///home/gavin/Machine/PySpark/Spark基础/data/text.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取词信息(从本地文件系统中读取数据)\n",
    "path = os.path.join(os.getcwd(), \"data/text.txt\")\n",
    "words = sc.textFile(name=\"file://\" + path, minPartitions=5)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Prior versions of the MovieLens dataset included either pre-computed cross-',\n",
       " 'folds or scripts to perform this computation. We no longer bundle either of',\n",
       " 'these features with the dataset, since most modern toolkits provide this as',\n",
       " ' a built-in feature. If you wish to learn about standard approaches to cross',\n",
       " '-fold computation in the context of recommender systems evaluation, see for',\n",
       " ' tools, documentation, and open-source code examples.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建整体流程(只构建，不计算)\n",
    "RDD = words.flatMap(lambda line:line.split(\" \")).map(lambda x:(x, 1)).reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词典大小\n",
    "len(RDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 3),\n",
       " ('to', 3),\n",
       " ('no', 1),\n",
       " ('', 2),\n",
       " ('learn', 1),\n",
       " ('computation', 1),\n",
       " ('see', 1),\n",
       " ('code', 1),\n",
       " ('examples.', 1),\n",
       " ('versions', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#聚合操作，也即整合所有的流程得到输出结果\n",
    "RDD.collect()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD的常用操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Transformation算子\n",
    "**目的**：从一个已经存在的数据集创建一个新的数据集  \n",
    "**说明**：  \n",
    "(1) 对于RDD而言，每一次转换操作都会产生不同的RDD，供给下一个“转换”使用.  \n",
    "(2) 转换得到的RDD是惰性求值的，也就是说，整个转换过程只是记录了转换的轨迹，并不会发生真正的计算，只有遇到行动操作时，才会发生真正的计算，开始从血缘关系源头开始，进行物理的转换操作．  \n",
    "**常用转换操作**：  \n",
    "![](./imgs/transform.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map(fun)\n",
    "rdd1 = sc.parallelize([1, 2, 3, 5, 6, 7, 8], 5)\n",
    "rdd2 = rdd1.map(lambda x: x+1)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 8, 9]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter(fun)\n",
    "rdd3 = rdd2.filter(lambda x:x>5)\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'x', 'd', 'u', 'j', 'l', 'p', 'o'] [['a', 'b', 'x'], ['d', 'u', 'j'], ['l', 'p', 'o']]\n"
     ]
    }
   ],
   "source": [
    "# flatMap操作, 先执行map操作，后执行flatc操作\n",
    "rdd4 = sc.parallelize([\"a b x\", \"d u j\", \"l p o\"], 4)\n",
    "rdd5 = rdd4.flatMap(lambda x: x.split(\" \"))\n",
    "rdd6 = rdd4.map(lambda x:x.split(\" \"))\n",
    "print(rdd5.collect(), rdd6.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 2), ('c', 1), ('b', 3)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# union 求并集\n",
    "rdd1 = sc.parallelize([(\"a\",1),(\"b\",2)])\n",
    "rdd2 = sc.parallelize([(\"c\",1),(\"b\",3)])\n",
    "rdd3 = rdd1.union(rdd2)\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', <pyspark.resultiterable.ResultIterable at 0x7f86f8df02b0>),\n",
       " ('b', <pyspark.resultiterable.ResultIterable at 0x7f86f8d8ab50>),\n",
       " ('c', <pyspark.resultiterable.ResultIterable at 0x7f86f8d8afd0>)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupByKey [(\"b\", [2, 3])]\n",
    "rdd4 = rdd3.groupByKey()\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(rdd4.collect()[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 5), ('c', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey，相同key值的value值相加\n",
    "rdd5 = rdd3.reduceByKey(lambda x, y:x+y)\n",
    "rdd5.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mary', 1), ('a', 3), ('had', 2), ('lamb', 5), ('little', 4)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortByKey\n",
    "temp = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
    "rdd = sc.parallelize(temp, 5)\n",
    "rdd1 = rdd.sortByKey()\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lamb', 5), ('little', 4), ('a', 3), ('had', 2), ('Mary', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortBy\n",
    "rdd2 = rdd.sortBy(keyfunc=lambda x:x[1], ascending=False)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Action算子\n",
    "**说明**：  \n",
    "行动操作是真正触发计算的地方。Spark程序执行到行动操作时，才会执行真正的计算，从文件中加载数据，完成一次又一次转换操作，最终，完成行动操作得到结果。  \n",
    "**惰性机制**:  \n",
    "所谓的“惰性机制”是指，整个转换过程只是记录了转换的轨迹，并不会发生真正的计算，只有遇到行动操作时，才会触发“从头到尾”的真正的计算。  \n",
    "**常用动作操作**:  \n",
    "![](./imgs/action.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Mary', 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first\n",
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mary', 1), ('had', 2)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take(显示指定个数的元素)\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('little', 4), ('lamb', 5)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.top(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.foreach(lambda x: print(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce累加\n",
    "rdd_re = sc.parallelize([2, 3, 5, 4])\n",
    "rdd_re.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 持久化\n",
    "**持久化的由来**:  \n",
    "在Spark中，RDD采用惰性求值的机制，每次遇到行动操作，都会从头开始执行计算。每次调用行动操作，都会触发一次从头开始的计算。这对于迭代计算而言，代价是很大的，迭代计算经常需要多次重复使用同一组数据,因此将元素多次使用的RDD保存到内存中，可以加快计算速度。  \n",
    "**说明**：  \n",
    "- 可以通过持久化（缓存）机制避免这种重复计算的开销\n",
    "- 可以使用persist()方法对一个RDD标记为持久化\n",
    "- 之所以说“标记为持久化”，是因为出现persist()语句的地方，并不会马上计算生成RDD并把它持久化，而是要等到遇到第一个行动操作触发真正计算以后，才会把计算结果进行持久化\n",
    "- 持久化后的RDD将会被保留在计算节点的内存中被后面的行动操作重复使用  \n",
    "**常用操作**：  \n",
    "![](./imgs/persist.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "name_array = [\"Hadoop\", \"Spark\", \"Hive\"]\n",
    "rdd = sc.parallelize(name_array)\n",
    "# 数据缓存到内存中，在遇见第一个行动操作时执行(可以使用rdd.cache() 保存到内存中)\n",
    "rdd.persist()\n",
    "print(rdd.count())  # 从头到尾执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop,Spark,Hive\n"
     ]
    }
   ],
   "source": [
    "print(\",\".join(rdd.collect()))  # 直接使用缓存在内存中的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[44] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不在使用时,从内存中释放\n",
    "rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分区\n",
    "**分区说明**:  \n",
    "RDD是弹性分布式数据集，通常RDD很大，会被分成很多个分区，分别保存在不同的节点上。  \n",
    "**分区的优点**:  \n",
    "（１）增加并行度  \n",
    "（２）减少通讯开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2], [3], [4], [5]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分区\n",
    "data = sc.parallelize([1, 2, 3, 4, 5], 5)\n",
    "data.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3, 4], [2, 5]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重新分区\n",
    "rdd = data.repartition(2)\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 1)], [(2, 1)], [(3, 1)], [(4, 1)], [(5, 1)]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = data.map(lambda x:(x, 1))\n",
    "rdd1.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd1.partitionBy(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(2, 1), (4, 1)], [(1, 1), (3, 1), (5, 1)]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义分区\n",
    "\n",
    "class SelfPartition:\n",
    "    \n",
    "    def myPartition(self, key):\n",
    "        \"\"\"自定义分区\"\"\"\n",
    "        \n",
    "        print(\"My partition is running!\")\n",
    "        print(\"The key is %d\"%key)\n",
    "        \n",
    "        return key%10\n",
    "    \n",
    "    def main(self):\n",
    "        \"\"\"自定义分区函数\"\"\"\n",
    "\n",
    "        print(\"The main funcation is running!\")\n",
    "        conf = SparkConf().setMaster(\"local\").setAppName(\"MyApp\")\n",
    "        sc = SparkContext(conf=conf)\n",
    "        \n",
    "        data = sc.parallelize(range(10), 5)\n",
    "        data.map(lambda x:(x, 1)).partitionBy(10, self.myPartition).map(lambda x:x[0]).saveAsTextFile(\"./data/pp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SelfPartition()\n",
    "sp.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 实战１：通过Spark实现点击流日志分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网络总访问量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"pv\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "path1 = os.path.join(os.getcwd(), \"data/access.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['194.237.142.21 - - [18/Sep/2013:06:49:18 +0000] \"GET /wp-content/uploads/2013/07/rstudio-git3.png HTTP/1.1\" 304 0 \"-\" \"Mozilla/4.0 (compatible;)\"',\n",
       " '183.49.46.228 - - [18/Sep/2013:06:49:23 +0000] \"-\" 400 0 \"-\" \"-\"',\n",
       " '163.177.71.12 - - [18/Sep/2013:06:49:33 +0000] \"HEAD / HTTP/1.1\" 200 20 \"-\" \"DNSPod-Monitor/1.0\"',\n",
       " '163.177.71.12 - - [18/Sep/2013:06:49:36 +0000] \"HEAD / HTTP/1.1\" 200 20 \"-\" \"DNSPod-Monitor/1.0\"',\n",
       " '101.226.68.137 - - [18/Sep/2013:06:49:42 +0000] \"HEAD / HTTP/1.1\" 200 20 \"-\" \"DNSPod-Monitor/1.0\"']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 网络信息\n",
    "rdd = sc.textFile(\"file://\"+path1)\n",
    "rdd.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pv', 90)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每一行数据表示一次网站访问(访问量)\n",
    "rdd1 = rdd.map(lambda x:(\"pv\", 1)).reduceByKey(lambda a, b :a+b)\n",
    "# rdd1.saveAsTextFile(\"data/pv.txt\")\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网站独立用户访问量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"uv\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "path1 = os.path.join(os.getcwd(), \"data/access.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['194.237.142.21',\n",
       " '183.49.46.228',\n",
       " '163.177.71.12',\n",
       " '163.177.71.12',\n",
       " '101.226.68.137']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得到ip地址\n",
    "rdd1 = sc.textFile(\"file://\" + path1)\n",
    "rdd2 = rdd1.map(lambda x:x.split(\" \")).map(lambda x:x[0])\n",
    "rdd2.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('uv', 17)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = rdd2.distinct().map(lambda x:(\"uv\", 1))\n",
    "rdd4 = rdd3.reduceByKey(lambda a,b:a+b)\n",
    "rdd4.collect()\n",
    "# rdd4.saveAsTextFile(\"data/uv.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 访问TopN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"TopN\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "path1 = os.path.join(os.getcwd(), \"data/access.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\"-\"', 27),\n",
       " ('\"http://blog.fens.me/vps-ip-dns/\"', 18),\n",
       " ('\"http://blog.fens.me/wp-content/themes/silesia/style.css\"', 7),\n",
       " ('\"http://blog.fens.me/nodejs-socketio-chat/\"', 7),\n",
       " ('\"http://blog.fens.me/nodejs-grunt-intro/\"', 7)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.textFile(\"file://\" + path1)\n",
    "rdd2 = rdd1.map(lambda line:line.split(\" \")).filter(lambda x:len(x)>10).map(lambda x:(x[10],1))\n",
    "rdd3 = rdd2.reduceByKey(lambda a, b:a+b).sortBy(lambda x:x[1], ascending=False)\n",
    "rdd4 = rdd3.take(5)\n",
    "rdd4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('\"-\"', 27),\n",
       "  ('\"http://blog.fens.me/vps-ip-dns/\"', 18),\n",
       "  ('\"http://blog.fens.me/wp-content/themes/silesia/style.css\"', 7),\n",
       "  ('\"http://blog.fens.me/nodejs-socketio-chat/\"', 7),\n",
       "  ('\"http://blog.fens.me/nodejs-grunt-intro/\"', 7)],\n",
       " [('\"http://blog.fens.me/nodejs-async/\"', 5),\n",
       "  ('\"http://www.angularjs.cn/A00n\"', 2),\n",
       "  ('\"http://cos.name/category/software/packages/\"', 1),\n",
       "  ('\"http://blog.fens.me/series-nodejs/\"', 1),\n",
       "  ('\"http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=6&cad=rja&ved=0CHIQFjAF&url=http%3A%2F%2Fblog.fens.me%2Fvps-ip-dns%2F&ei=j045UrP5AYX22AXsg4G4DQ&usg=AFQjCNGsJfLMNZnwWXNpTSUl6SOEzfF6tg&sig2=YY1oxEybUL7wx3IrVIMfHA&bvm=bv.52288139,d.b2I\"',\n",
       "   1),\n",
       "  ('\"http://www.google.com/url?sa=t&rct=j&q=nodejs%20%E5%BC%82%E6%AD%A5%E5%B9%BF%E6%92%AD&source=web&cd=1&cad=rja&ved=0CCgQFjAA&url=%68%74%74%70%3a%2f%2f%62%6c%6f%67%2e%66%65%6e%73%2e%6d%65%2f%6e%6f%64%65%6a%73%2d%73%6f%63%6b%65%74%69%6f%2d%63%68%61%74%2f&ei=rko5UrylAefOiAe7_IGQBw&usg=AFQjCNG6YWoZsJ_bSj8kTnMHcH51hYQkAA&bvm=bv.52288139,d.aGc\"',\n",
       "   1),\n",
       "  ('\"http://www.angularjs.cn/\"', 1)]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得到partion的分组情况\n",
    "rdd3.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 终止\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实战2：实现ip地址查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"IPSearch\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "ip_path = os.path.join(os.getcwd(), \"data/ip.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_id_rdd = sc.textFile(\"file://\" + ip_path)\n",
    "city_id_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('16777472', '16778239', '福州', '119.306239', '26.075302'),\n",
       " ('16779264', '16781311', '广州', '113.280637', '23.125178'),\n",
       " ('16785408', '16793599', '广州', '113.280637', '23.125178'),\n",
       " ('16842752', '16843007', '福州', '119.306239', '26.075302'),\n",
       " ('16843264', '16844799', '福州', '119.306239', '26.075302')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 起始地址 截止地址  经度 纬度\n",
    "city_id_rdd = city_id_rdd.map(lambda x:x.split(\"|\")).map(lambda x:(x[2], x[3],x[7], x[13], x[14]))\n",
    "city_id_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('16777472', '16778239', '福州', '119.306239', '26.075302'),\n",
       " ('16779264', '16781311', '广州', '113.280637', '23.125178')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#　创建广播变量，使其不需要复制到每一个数据集当中\n",
    "city_brodcast = sc.broadcast(city_id_rdd.collect())\n",
    "city_brodcast.value[:2]  # 列表形式存取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['125.213.100.123',\n",
       " '117.101.215.133',\n",
       " '117.101.222.68',\n",
       " '115.120.36.118',\n",
       " '123.197.64.247']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获得相对应的ip地址\n",
    "dest_data = sc.textFile(\"data/20090121000132.394251.http.format\").map(lambda x:x.split(\"|\")[1])\n",
    "dest_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ip_transform(ip):\n",
    "    \"\"\"ip地址装换\"\"\"\n",
    "    \n",
    "    ips = ip.split(\".\")#[223,243,0,0] 32位二进制数\n",
    "    ip_num = 0\n",
    "    for i in ips:\n",
    "        ip_num = int(i) | ip_num << 8\n",
    "    return ip_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search(ip_num, broadcast_value):\n",
    "    start = 0\n",
    "    end = len(broadcast_value) - 1\n",
    "    while (start <= end):\n",
    "        mid = int((start + end) / 2)\n",
    "        if ip_num >= int(broadcast_value[mid][0]) and ip_num <= int(broadcast_value[mid][1]):\n",
    "            return mid\n",
    "        if ip_num < int(broadcast_value[mid][0]):\n",
    "            end = mid\n",
    "        if ip_num > int(broadcast_value[mid][1]):\n",
    "            start = mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(x):\n",
    "    \"\"\"ip地址\"\"\"\n",
    "    \n",
    "    city_brodcast_value = city_brodcast.value\n",
    "    def get_result(ip):\n",
    "        ip_num = ip_transform(ip)\n",
    "        # index表示相对应的ip的位置信息的索引结点\n",
    "        index = binary_search(ip_num, city_brodcast_value)\n",
    "        \n",
    "        # ((经度, 纬度, 地址), 1))\n",
    "        return ((city_brodcast_value[index][2], city_brodcast_value[index][3], city_brodcast_value[index][4]), 1)\n",
    "    \n",
    "    x = map(tuple, [get_result(ip) for ip in x])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照Partitiion传入数据块进行运算\n",
    "dest_rdd = dest_data.mapPartitions(lambda x:get_pos(x))\n",
    "result_rdd = dest_rdd.reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('西安', '108.948024', '34.263161'), 1824),\n",
       " (('重庆', '107.7601', '29.32548'), 85),\n",
       " (('重庆', '106.504962', '29.533155'), 400),\n",
       " (('重庆', '106.27633', '29.97227'), 36),\n",
       " (('重庆', '107.39007', '29.70292'), 47),\n",
       " (('重庆', '106.56347', '29.52311'), 3),\n",
       " (('重庆', '107.08166', '29.85359'), 29),\n",
       " (('北京', '116.405285', '39.904989'), 1535),\n",
       " (('石家庄', '114.502461', '38.045474'), 383),\n",
       " (('重庆', '106.51107', '29.50197'), 91),\n",
       " (('昆明', '102.712251', '25.040609'), 126),\n",
       " (('重庆', '106.57434', '29.60658'), 177)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 统计相同地址经纬度下的sun\n",
    "result_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair RDD操作\n",
    "说明：统计每种图书每天的平均销量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 2), ('hadoop', 6), ('hadoop', 8), ('Spark', 5)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"Spark\", 2), (\"hadoop\", 6), (\"hadoop\", 8), (\"Spark\", 5)])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hadoop', (14, 2)), ('Spark', (7, 2))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hadoop', 0.14285714285714285), ('Spark', 0.2857142857142857)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.mapValues(lambda x: x[1]/x[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求TOP\n",
    "file文件有4列，求第三列前N个TOP值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import os\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 路径\n",
    "PATH = os.path.join(os.getcwd(), \"data/top.txt\")\n",
    "# 配置运行环境\n",
    "conf = SparkConf().setAppName(\"TOP\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,1768,50,155',\n",
       " '2,1218, 600,211',\n",
       " '3,2239,788,242',\n",
       " '4,3101,28,599',\n",
       " '5,4899,290,129',\n",
       " '6,3110,54,1201',\n",
       " '7,4436,259,877',\n",
       " '8,2369,7890,27']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"file://\" + PATH)\n",
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7890, 788, 600, 290, 259]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 消除空格，并且一行满足四个特征\n",
    "result1 = lines.filter(lambda line:(len(line.strip()) > 0) and (len(line.split(\",\")) == 4))\n",
    "result2 = result1.map(lambda line: line.split(\",\")[2])\n",
    "# 构建键值对元素\n",
    "result3 = result2.map(lambda x: (int(x), \"\"))\n",
    "# 整合为一个分区，全局最值\n",
    "result4 = result3.repartition(1)\n",
    "# 降序排列\n",
    "result5 = result4.sortByKey(False)\n",
    "result6 = result5.map(lambda x: x[0])\n",
    "result6.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文件排序\n",
    "读取文件中所有整数进行排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "PATH = os.path.join(os.getcwd(), \"data/FileSort.txt\")\n",
    "PATH\n",
    "\n",
    "def getIndex():\n",
    "    \n",
    "    global index\n",
    "    index += 1\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(path):\n",
    "    \n",
    "    conf = SparkConf().setAppName(\"file_sort\").setMaster(\"local\")\n",
    "    sc = SparkContext(conf=conf)\n",
    "    rdd = sc.textFile(\"file://\" + path)\n",
    "    \n",
    "    result1 = rdd.filter(lambda line: (len(line.strip()) > 0))\n",
    "    # 构建键值对元素\n",
    "    result2 = result1.map(lambda x: (int(x.strip()), \"\"))\n",
    "    # 整合为一个分区，全局最值\n",
    "    result3 = result2.repartition(1)\n",
    "    # 降序排列\n",
    "    result4 = result3.sortByKey(False)\n",
    "    result5 = result4.map(lambda x: x[0])\n",
    "    # rank\n",
    "    result6 = result5.map(lambda x: (getIndex(), x))\n",
    "    # 只包含文件目录\n",
    "    result6.saveAsTextFile(\"file://\" + os.getcwd() + \"/data/FileSort\")\n",
    "    \n",
    "    return result6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4899), (2, 4436), (3, 3110), (4, 3101)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = main(PATH)\n",
    "res.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二次排序\n",
    "对于一个给定的文件（有两列整数），请对数据进行排序，首先根据第一列数据降序排序，如果第一列数据相等，则根据第二列数据降序排序\n",
    "- 按照Ordered和Serializable接口实现自定义排序的key\n",
    "- 将要进行二次排序的文件加载进来生成<key,value>类型的RDD\n",
    "- 使用sortByKey基于自定义的key进行二次排序\n",
    "- 去掉排序的key只保留排序的结果  \n",
    "**解题思路：**  \n",
    "![](./imgs/sort.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import gt\n",
    "\n",
    "\n",
    "# 路径\n",
    "PATH = os.path.join(os.getcwd(), \"data/file.txt\")\n",
    "# 环境配置\n",
    "conf = SparkConf().setAppName(\"spark_sort\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondarySortKey():\n",
    "    \n",
    "    def __init__(self,k):\n",
    "        self.column1 = k[0]\n",
    "        self.column2 = k[1]\n",
    "\n",
    "    def __gt__(self, other):\n",
    "\n",
    "        if other.column1 ==self.column1:\n",
    "\n",
    "            return gt(self.column2,other.column2)\n",
    "        else:\n",
    "            return gt(self.column1,other.column1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['50 155',\n",
       " '600 211',\n",
       " '788 242',\n",
       " '28 599',\n",
       " '290 129',\n",
       " '54 1201',\n",
       " '259 877',\n",
       " '7890 27']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"file://\" + PATH)\n",
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((50, 155), '50 155'),\n",
       " ((600, 211), '600 211'),\n",
       " ((788, 242), '788 242'),\n",
       " ((28, 599), '28 599'),\n",
       " ((290, 129), '290 129'),\n",
       " ((54, 1201), '54 1201'),\n",
       " ((259, 877), '259 877'),\n",
       " ((7890, 27), '7890 27')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = lines.filter(lambda x: (len(x.strip()) > 0))\n",
    "# 构建可排序的key\n",
    "rdd2 = rdd1.map(lambda x: ((int(x.split(\" \")[0]), int(x.split(\" \")[1])), x))\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7890 27',\n",
       " '788 242',\n",
       " '600 211',\n",
       " '290 129',\n",
       " '259 877',\n",
       " '54 1201',\n",
       " '50 155',\n",
       " '28 599']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = rdd2.map(lambda x: (SecondarySortKey(x[0]), x[1]))\n",
    "rdd4 = rdd3.sortByKey(False)\n",
    "rdd5 = rdd4.map(lambda x: x[1])\n",
    "rdd5.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ubuntu18.04安装spark（伪分布式）](https://blog.csdn.net/weixin_42001089/article/details/82346367#commentBox)  \n",
    "[jupyter－lab使用pyspark](https://blog.csdn.net/syani/article/details/72851425)  \n",
    "[Spark词频统计，求TOP值，文件排序，二次排序](https://www.cnblogs.com/chenshaowei/p/12435335.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
