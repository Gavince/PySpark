{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark之SparkMLlib基本操作\n",
    "## 前言\n",
    "Spark的引入:  \n",
    "- 传统的机器学习算法，由于技术和单机存储的限制，只能在少量数据上使用，依赖于数据抽样\n",
    "- 大数据技术的出现，可以支持在全量数据上进行机器学习\n",
    "- 机器学习算法涉及大量迭代计算\n",
    "- 基于磁盘的MapReduce不适合进行大量迭代计算\n",
    "- 基于内存的Spark比较适合进行大量迭代计算  \n",
    "\n",
    "Spark的优点:  \n",
    "- Spark提供了一个基于海量数据的机器学习库，它提供了常用机器学习算法的分布式实现\n",
    "- 开发者只需要有Spark 基础并且了解机器学习算法的原理，以及方法相关参数的含义，就可以轻松的通过调用相应的API 来实现基于海量数据的机器学习过程\n",
    "- pyspark的即时查询也是一个关键。算法工程师可以边写代码边运行，边看结果  \n",
    "\n",
    "MLlib:  \n",
    "- MLlib是Spark的机器学习（Machine Learning）库，旨在简化机器学习的工程实践工作\n",
    "- MLlib由一些通用的学习算法和工具组成，包括分类、回归、聚类、协同过滤、降维等，同时还包括底层的优化原语和高层的流水线（Pipeline）API，具体如下：\n",
    "    1. 算法工具：常用的学习算法，如分类、回归、聚类和协同过滤；\n",
    "    2. 特征化工具：特征提取、转化、降维和选择工具；\n",
    "    3. 流水线(Pipeline)：用于构建、评估和调整机器学习工作流的工具;\n",
    "    4. 持久性：保存和加载算法、模型和管道;\n",
    "    5. 实用工具：线性代数、统计、数据处理等工具。  \n",
    "    \n",
    "MLlib和ML的区别:  \n",
    "- spark.mllib包含基于RDD的原始算法API。Spark MLlib 历史比较长，在1.0 以前的版本即已经包含了，提供的算法实现都是基于原始的RDD\n",
    "- spark.ml 则提供了基于DataFrames 高层次的API，可以用来构建机器学习工作流（PipeLine）。ML Pipeline 弥补了原始MLlib 库的不足，向用户提供了一个基于DataFrame 的机器学习工作流式API 套件\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习流水线\n",
    "Pipline的使用：一个管道可以被认为由一系列不同阶段组成。在Pipeline对象上执行.fit()方法时，所有阶段按照stages参数中指定的顺序执行；stages参数是转换器和评估器对象的列表。管道对象的.fit()方法执行每个转化器的.transform()方法和所有评估器的.fit()方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IDF\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.191.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>word_count</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb6841d2340>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"word_count\").master(\"local\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+\n",
      "| id|            text|label|\n",
      "+---+----------------+-----+\n",
      "|  0| a b c d e spark|  1.0|\n",
      "|  1|             b d|  0.0|\n",
      "|  2|     spark f g h|  1.0|\n",
      "|  3|hadoop mapreduce|  0.0|\n",
      "+---+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training = spark.createDataFrame([(0, \"a b c d e spark\", 1.0)\n",
    "                                  ,(1, \"b d\", 0.0)\n",
    "                                  ,(2, \"spark f g h\", 1.0)\n",
    "                                  ,(3, \"hadoop mapreduce\", 0.0)],\n",
    "                                 [\"id\", \"text\", \"label\"]\n",
    "                                )\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tokenizer that converts the input string to lowercase and then\n",
    "# splits it by white spaces.(字符串转为小写，并split())\n",
    "# 实例化\n",
    "token = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=token.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "# 管道化\n",
    "pipline = Pipeline(stages=[token, hashingTF, lr])\n",
    "model = pipline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rid: 4, text: (spark i j k), prob: [0.1596407738787412,0.8403592261212588], pred: 1.0\n",
      "rid: 5, text: (l m n), prob: [0.8378325685476614,0.16216743145233858], pred: 0.0\n",
      "rid: 6, text: (spark hadoop spark), prob: [0.06926633132976266,0.9307336686702373], pred: 1.0\n",
      "rid: 7, text: (apache hadoop), prob: [0.9821575333444208,0.017842466655579203], pred: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 测试数据集\n",
    "test = spark.createDataFrame([(4, \"spark i j k\")\n",
    "                              ,(5, \"l m n\")\n",
    "                              ,(6, \"spark hadoop spark\")\n",
    "                              ,(7, \"apache hadoop\")]\n",
    "                             , [\"id\", \"text\"]\n",
    "                            )\n",
    "\n",
    "# 预测\n",
    "predict = model.transform(test)\n",
    "select  = predict.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "\n",
    "for row in select.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"rid: {}, text: ({}), prob: {}, pred: {}\".format(rid, text, prob, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征装换\n",
    "### TF-IDF\n",
    "知识点:  \n",
    "![](./imgs/tf-idf.png)  \n",
    "过程描述：  \n",
    "1. 首先使用分解器Tokenizer把句子划分为单个词语\n",
    "2. 对每一个句子（词袋），使用HashingTF将句子转换为特征向量\n",
    "3. 最后使用IDF重新调整特征向量（这种转换通常可以提高使用文本特征的性能）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceData = spark.createDataFrame([(0, \"I heard about Spark and I love Spark\")\n",
    "                                      ,(0, \"I wish Java could use case classes\")\n",
    "                                      ,(1, \"Logistic regression models are neat\")\n",
    "                                     ]).toDF(\"label\", \"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            sentence|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|I heard about Spa...|[i, heard, about,...|\n",
      "|    0|I wish Java could...|[i, wish, java, c...|\n",
      "|    1|Logistic regressi...|[logistic, regres...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分词\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "tokenDF = tokenizer.transform(sentenceData)\n",
    "tokenDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+---------------------------------------------------------------------+\n",
      "|words                                        |rawfeatures                                                          |\n",
      "+---------------------------------------------+---------------------------------------------------------------------+\n",
      "|[i, heard, about, spark, and, i, love, spark]|(2000,[240,673,891,956,1286,1756],[1.0,1.0,1.0,1.0,2.0,2.0])         |\n",
      "|[i, wish, java, could, use, case, classes]   |(2000,[80,342,495,1133,1307,1756,1967],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|[logistic, regression, models, are, neat]    |(2000,[286,763,1059,1604,1871],[1.0,1.0,1.0,1.0,1.0])                |\n",
      "+---------------------------------------------+---------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hashingTF：　TF表示t在文档中存在的次数\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawfeatures\", numFeatures=2000)\n",
    "hashingDF = hashingTF.transform(tokenDF)\n",
    "hashingDF.select(\"words\", \"rawfeatures\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(2000,[240,673,89...|\n",
      "|(2000,[80,342,495...|\n",
      "|(2000,[286,763,10...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "idf = IDF(inputCol=\"rawfeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(hashingDF)\n",
    "rescale_feature = idfModel.transform(hashingDF)\n",
    "rescale_feature.select(\"features\").show(truncate = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StringIndexer字符转索引(LabelIndex)\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import IndexToString, VectorIndexer, OneHotEncoder\n",
    "from pyspark.ml.linalg import Vector, Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+\n",
      "| id|category|category2index|\n",
      "+---+--------+--------------+\n",
      "|  0|       a|           0.0|\n",
      "|  1|       b|           2.0|\n",
      "|  2|       c|           1.0|\n",
      "|  3|       a|           0.0|\n",
      "|  4|       a|           0.0|\n",
      "|  5|       c|           1.0|\n",
      "+---+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "df = spark.createDataFrame([(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")]\n",
    "                           ,[\"id\", \"category\"]\n",
    "                          )\n",
    "\n",
    "# index\n",
    "string2index = StringIndexer(inputCol=\"category\", outputCol=\"category2index\")\n",
    "model_string = string2index.fit(df)\n",
    "df_index = model_string.transform(df)\n",
    "df_index.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IndexToString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+--------+\n",
      "| id|category|category2index|original|\n",
      "+---+--------+--------------+--------+\n",
      "|  0|       a|           0.0|       a|\n",
      "|  1|       b|           2.0|       b|\n",
      "|  2|       c|           1.0|       c|\n",
      "|  3|       a|           0.0|       a|\n",
      "|  4|       a|           0.0|       a|\n",
      "|  5|       c|           1.0|       c|\n",
      "+---+--------+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 还原回原来的的字符类别索引\n",
    "index2string = IndexToString(inputCol=\"category2index\", outputCol=\"original\")\n",
    "df_original = index2string.transform(df_index)\n",
    "df_original.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+--------+-------------+\n",
      "| id|category|category2index|original|  onthot_cate|\n",
      "+---+--------+--------------+--------+-------------+\n",
      "|  0|       a|           0.0|       a|(2,[0],[1.0])|\n",
      "|  1|       b|           2.0|       b|    (2,[],[])|\n",
      "|  2|       c|           1.0|       c|(2,[1],[1.0])|\n",
      "|  3|       a|           0.0|       a|(2,[0],[1.0])|\n",
      "|  4|       a|           0.0|       a|(2,[0],[1.0])|\n",
      "|  5|       c|           1.0|       c|(2,[1],[1.0])|\n",
      "+---+--------+--------------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 必须为数值型编码\n",
    "onehot = OneHotEncoder(inputCol=\"category2index\", outputCol=\"onthot_cate\")\n",
    "onehot = onehot.fit(df_original)\n",
    "onehot_df = onehot.transform(df_original)\n",
    "onehot_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|      features|   vect2index|\n",
      "+--------------+-------------+\n",
      "|[-1.0,1.0,1.0]|[1.0,1.0,0.0]|\n",
      "|[-1.0,3.0,1.0]|[1.0,3.0,0.0]|\n",
      "| [0.0,5.0,1.0]|[0.0,5.0,0.0]|\n",
      "+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 每一个vector是一个样本的特征向量，纵向编码\n",
    "df = spark.createDataFrame([(Vectors.dense(-1.0, 1.0, 1.0),)\n",
    "                            , (Vectors.dense(-1.0, 3.0, 1.0),)\n",
    "                            , (Vectors.dense(0.0, 5.0, 1.0), )]\n",
    "                           , [\"features\"]\n",
    "                          )\n",
    "\n",
    "# maxCategories表示超过此值后，不进行类别编码，dense_feat\n",
    "vect2index = VectorIndexer(maxCategories=2, inputCol=\"features\", outputCol=\"vect2index\")\n",
    "vect_model = vect2index.fit(df)\n",
    "df_vect = vect_model.transform(df)\n",
    "df_vect.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR 和 RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.types import FloatType, StringType, StructField, StructType\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "\n",
    "ROOT = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width| label|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "|         5.1|        3.5|         1.4|        0.2|setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2|setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2|setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2|setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2|setosa|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([StructField(\"sepal_length\", FloatType())\n",
    "                     , StructField(\"sepal_width\", FloatType())\n",
    "                     , StructField(\"petal_length\", FloatType())\n",
    "                     , StructField(\"petal_width\", FloatType())\n",
    "                     , StructField(\"label\", StringType())\n",
    "                    ]\n",
    "                   )\n",
    "\n",
    "df_iris = spark.read.csv(\"file://\" + ROOT + \"/data/iris.csv\", schema=schema, header=True,)\n",
    "df_iris.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: float (nullable = true)\n",
      " |-- sepal_width: float (nullable = true)\n",
      " |-- petal_length: float (nullable = true)\n",
      " |-- petal_width: float (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A feature transformer that merges multiple columns into a vector column.\n",
    "vector = VectorAssembler(inputCols=df_iris.columns[:-1], outputCol=\"features\")\n",
    "vector_df = vector.transform(df_iris)\n",
    "\n",
    "# 类别编码\n",
    "string_model = StringIndexer(inputCol=\"label\", outputCol=\"index_label\")\n",
    "string_model = string_model.fit(vector_df)\n",
    "string_df = string_model.transform(vector_df)\n",
    "\n",
    "# 构成特征vector, label\n",
    "df = string_df.select(\"features\", \"index_label\")\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+\n",
      "|            features|prediction|     label|\n",
      "+--------------------+----------+----------+\n",
      "|[4.40000009536743...|       0.0|    setosa|\n",
      "|[4.59999990463256...|       0.0|    setosa|\n",
      "|[4.59999990463256...|       0.0|    setosa|\n",
      "|[4.80000019073486...|       0.0|    setosa|\n",
      "|[4.90000009536743...|       1.0|versicolor|\n",
      "+--------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 训练LR模型\n",
    "train_data, test_data = df.randomSplit([0.7, 0.3])\n",
    "lr = LogisticRegression(featuresCol=\"features\"\n",
    "                        , labelCol=\"index_label\"\n",
    "                        , maxIter=100\n",
    "                        , regParam=0.1\n",
    "                        , elasticNetParam=0.8\n",
    "                       )\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# index label转为真实索引值，测试集无标签\n",
    "predict = model.transform(test_data)\n",
    "\n",
    "index2string = IndexToString(inputCol=\"prediction\", outputCol=\"label\", labels=string_model.labels)\n",
    "predict_df = index2string.transform(predict)\n",
    "predict_df.select(\"features\", \"prediction\", \"label\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.828395061728395"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 多分类的评估\n",
    "multi_eval = MulticlassClassificationEvaluator(predictionCol=\"prediction\"\n",
    "                                               , labelCol=\"index_label\"\n",
    "                                              )\n",
    "multi_eval.evaluate(predict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(3, 4, [0.0, 0.4267, -0.5337, -0.7243, 0.0, -0.5844, 0.0, 0.0, 0.0, 0.0, 0.1242, 1.1708], 1)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coefficientMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.numClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.numFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考\n",
    "[林子雨主讲《大数据处理技术Spark》](http://dblab.xmu.edu.cn/post/12157/#kejianxiazai)  \n",
    "[TF-IDF原理及使用](https://blog.csdn.net/zrc199021/article/details/53728499)  \n",
    "[PySpark学习笔记-HashingTF()方法原理](https://blog.csdn.net/yeshang_lady/article/details/90200170)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
