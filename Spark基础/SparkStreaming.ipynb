{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkStreaming\n",
    "\n",
    "它是一个可扩展，高吞吐具有容错性的流式计算框架  \n",
    "**吞吐量**：单位时间内成功传输数据的数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词频统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sscrun.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sscrun.py\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# 创建环境\n",
    "sc = SparkContext(master=\"local[2]\", appName=\"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 1)  # 每隔一秒钟监听一次数据\n",
    "\n",
    "# 监听端口数据\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "#　拆分单词\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "# 统计词频\n",
    "pairs = words.map(lambda word:(word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y:x+y)\n",
    "# 打印数据\n",
    "wordCounts.pprint()\n",
    "\n",
    "# 开启流式处理\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**wordCounts.pprint??**\n",
    "```python\n",
    "Signature: wordCounts.pprint(num=10)\n",
    "Source:   \n",
    "    def pprint(self, num=10):\n",
    "        \"\"\"\n",
    "        Print the first num elements of each RDD generated in this DStream.\n",
    "\n",
    "        :param num: the number of elements from the first will be printed.\n",
    "        \"\"\"\n",
    "        def takeAndPrint(time, rdd):\n",
    "            taken = rdd.take(num + 1)\n",
    "            print(\"-------------------------------------------\")\n",
    "            print(\"Time: %s\" % time)\n",
    "            print(\"-------------------------------------------\")\n",
    "            for record in taken[:num]:\n",
    "                print(record)\n",
    "            if len(taken) > num:\n",
    "                print(\"...\")\n",
    "            print(\"\")\n",
    "\n",
    "        self.foreachRDD(takeAndPrint)\n",
    "File:      ~/anaconda3/lib/python3.6/site-packages/pyspark/streaming/dstream.py\n",
    "Type:      method\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Streaming有状态操作\n",
    "\n",
    "%more sscrun.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing calcultor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile calcultor.py\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# 创建环境\n",
    "sc = SparkContext(master=\"local[2]\", appName=\"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 3)  # 每隔一秒钟监听一次数据\n",
    "\n",
    "# 设置检测点\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "# 状态更新函数\n",
    "def updatefun(new_values,last_sum):\n",
    "    \n",
    "    # 向前转态加上当前状态的key状态的value值\n",
    "    return sum(new_values) + (last_sum or 0)\n",
    "\n",
    "# 监听端口数据\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "#　拆分单词\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "#  统计词频\n",
    "pairs = words.map(lambda word:(word, 1))\n",
    "wordCounts = pairs.updateStateByKey(updateFunc=updatefun)\n",
    "wordCounts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top.py\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "def get_countryname(line):\n",
    "    country_name = line.strip()\n",
    "\n",
    "    if country_name == 'usa':\n",
    "        output = 'USA'\n",
    "    elif country_name == 'ind':\n",
    "        output = 'India'\n",
    "    elif country_name == 'aus':\n",
    "        output = 'Australia'\n",
    "    else:\n",
    "        output = 'Unknown'\n",
    "\n",
    "    return (output, 1)\n",
    "\n",
    "# 设置参数\n",
    "batch_interval = 1\n",
    "window_length = 6*batch_interval\n",
    "frquency = 3*batch_interval\n",
    "\n",
    "sc =  sc = SparkContext(master=\"local[2]\", appName=\"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "# 监听端口数据\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "addFunc = lambda x, y: x+y\n",
    "invAddFunc = lambda x, y: x-y\n",
    "word_counts = lines.map(get_countryname).reduceByKeyAndWindow(addFunc, invAddFunc, window_length, frquency)\n",
    "word_counts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
