{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark之Structured Streaming基本操作\n",
    "思想：将实时数据流视为一张正在不断添加的数据的表，可以把流计算等同于在一个静态表上的批处理查询，Spark会在不断添加数据的无界输入表上运行计算，并进行增量查询。  \n",
    "\n",
    "编写Structured Streaming程序的基本步骤包括：  \n",
    "- 导入pyspark模块\n",
    "- 创建SparkSession对象\n",
    "- 创建输入数据源\n",
    "- 定义流计算过程\n",
    "- 启动流计算并输出结果\n",
    "\n",
    "两种处理模型：  \n",
    "(1) 微批处理  \n",
    "(2) 持续处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词频统计\n",
    "目标：一个包含很多英文语句的数据流远远不断到达，Structured Streaming程序对每行英文语句进行拆分，并统计每个单词出现的频率。  \n",
    "![](./imgs/out_struct.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting structurednetworkwordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile structurednetworkwordcount.py\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark import Row\n",
    "from pyspark.sql.functions import split, length\n",
    "from pyspark.sql.functions import explode\n",
    "import os \n",
    "\n",
    "ROOT = \"file://\" + os.getcwd()\n",
    "\n",
    "\n",
    "# 创建SparkSession对象\n",
    "spark = SparkSession.builder.appName(\"StructuredNetworkWordCount\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# 创建输入数据源\n",
    "lines = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# 定义流计算过程\n",
    "words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "word_count = words.filter(length(\"word\") == 5)\n",
    "\n",
    "# 启动流计算并输出结果\n",
    "query = word_count \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", ROOT + \"/data/filesink\") \\\n",
    "    .option(\"checkpointLocation\", ROOT + \"/data/file-sink-cp\") \\\n",
    "    .trigger(processingTime=\"8 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文件读写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [08:22<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "TEST_DATA_TEMP_DIR = \"./data/tmp/\"\n",
    "TEST_DATA_DIR = \"./data/tmp/testdata/\"\n",
    "ACTION_DEF = [\"login\", \"logout\", \"purchase\"]\n",
    "DISTRICT_DEF = [\"fujian\", \"beijin\", \"shanghai\", \"guangzhou\"]\n",
    "JSIN_LINE_PATTERN = '{{\"eventTime\": {}, \"action\": {}, \"district\":{}}}\\n'\n",
    "\n",
    "def testSetUp():\n",
    "    \"\"\"创建临时文件目录\"\"\"\n",
    "    \n",
    "    if os.path.exists(TEST_DATA_DIR):\n",
    "        shutil.rmtree(TEST_DATA_DIR, ignore_errors=True)\n",
    "        \n",
    "    os.mkdir(TEST_DATA_DIR)\n",
    "    \n",
    "def testTearDown():\n",
    "    \"\"\"恢复测试环境\"\"\"\n",
    "    \n",
    "    if os.path.exists(TEST_DATA_DIR):\n",
    "        shutil.rmtree(TEST_DATA_DIR, ignore_errors=True)\n",
    "\n",
    "def writeAndMove(filename, data):\n",
    "    \"\"\"生成测试文件\"\"\"\n",
    "    \n",
    "    with open(TEST_DATA_TEMP_DIR + filename, \"wt\", encoding=\"utf-8\") as f:\n",
    "        f.write(data)\n",
    "    \n",
    "    shutil.move(TEST_DATA_TEMP_DIR + filename, TEST_DATA_DIR + filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    testSetUp()\n",
    "    \n",
    "    for i in tqdm(range(500)):\n",
    "        \n",
    "        filename = 'e-mall-{}.json'.format(i)\n",
    "        content = ''\n",
    "        rndcount = list(range(100))\n",
    "        random.shuffle(rndcount)\n",
    "        \n",
    "        for _ in rndcount:\n",
    "            content += JSIN_LINE_PATTERN.format(str(int(time.time()))\n",
    "                                                , random.choice(ACTION_DEF)\n",
    "                                                , random.choice(DISTRICT_DEF)\n",
    "                                               )\n",
    "        writeAndMove(filename, content)\n",
    "        time.sleep(1)\n",
    "        \n",
    "#     testTearDown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spark_ss_filesource.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spark_ss_filesource.py\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark import Row\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.types import StructField, StructType\n",
    "from pyspark.sql.types import StringType, TimestampType\n",
    "from pyspark.sql.functions import window, asc\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    \n",
    "    TEST_DATA_DIR_SPARK = \"file://\" + os.getcwd() + \"/data/tmp/testdata/\"\n",
    "    schema = StructType([StructField(\"eventTime\", TimestampType(), True)\n",
    "                         , StructField(\"action\", StringType(), True)\n",
    "                         , StructField(\"district\", StringType(), True)\n",
    "                        ])\n",
    "\n",
    "    # 创建SparkSession对象\n",
    "    spark = SparkSession.builder.appName(\"StructuredEmallPurchaseCount.py\").getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # 创建输入数据源\n",
    "    lines = spark.readStream \\\n",
    "        .format(\"json\") \\\n",
    "        .schema(schema) \\\n",
    "        .option(\"maxFilesPerTrigger\", 100) \\\n",
    "        .load(TEST_DATA_DIR_SPARK)\n",
    "\n",
    "    # 定义流计算过程\n",
    "    # 定义窗口：在指定的窗口时间内进行统计\n",
    "    windowDuration = \"1 minutes\"\n",
    "    words = lines.filter(lines.action == \"purchase\").groupBy(\"district\", window(\"eventTime\", windowDuration)).count().sort(asc(\"window\"))\n",
    "\n",
    "    # 启动流计算并输出结果\n",
    "    query = words \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .start()\n",
    "\n",
    "    query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 知识点\n",
    "### explode的用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+\n",
      "|  a|  intlist|mapfield|\n",
      "+---+---------+--------+\n",
      "|  1|[1, 2, 3]|{a -> b}|\n",
      "+---+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
    "eDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|anInt|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eDF.select(explode(eDF.intlist).alias(\"anInt\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+\n",
      "|  a|  intlist|mapfield|\n",
      "+---+---------+--------+\n",
      "|  1|[1, 2, 3]|{a -> b}|\n",
      "+---+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eDF.filter(eDF.a == 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考\n",
    "[Hadoop上传文件报错could only be written to 0 of the 1 minReplication nodes.](https://blog.csdn.net/sinat_38737592/article/details/101628357)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
