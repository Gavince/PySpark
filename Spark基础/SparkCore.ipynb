{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkCore\n",
    "基于内存的计算引擎，它的计算速度非常快。但是仅仅只涉及到数据的计算，并没有涉及到数据的存储，但是，spark的缺点是：吃内存，不太稳定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总体而言，Spark采用RDD以后能够实现高效计算的主要原因如下：  \n",
    "（1）高效的容错性。现有的分布式共享内存、键值存储、内存数据库等，为了实现容错，必须在集群节点之间进行数据复制或者记录日志，也就是在节点之间会发生大量的数据传输，这对于数据密集型应用而言会带来很大的开销。在RDD的设计中，数据只读，不可修改，如果需要修改数据，必须从父RDD转换到子RDD，由此在不同RDD之间建立了血缘关系。<font color=\"blue\">所以，RDD是一种天生具有容错机制的特殊集合，不需要通过数据冗余的方式（比如检查点）实现容错，而只需通过RDD父子依赖（血缘）关系重新计算得到丢失的分区来实现容错，无需回滚整个系统，这样就避免了数据复制的高开销，而且重算过程可以在不同节点之间并行进行，实现了高效的容错。</font>此外，RDD提供的转换操作都是一些粗粒度的操作（比如map、filter和join），RDD依赖关系只需要记录这种粗粒度的转换操作，而不需要记录具体的数据和各种细粒度操作的日志（比如对哪个数据项进行了修改），这就大大降低了数据密集型应用中的容错开销；  \n",
    "（2）中间结果持久化到内存。数据在内存中的多个RDD操作之间进行传递，不需要“落地”到磁盘上，避免了不必要的读写磁盘开销；  \n",
    "（3）存放的数据可以是Java对象，避免了不必要的对象序列化和反序列化开销。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入门(词频统计)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个程序的执行过程如下：\n",
    "*  创建这个Spark程序的执行上下文，即创建SparkContext对象；\n",
    "*  从外部数据源或HDFS文件中读取数据创建words对象；\n",
    "*  构建起words和RDD之间的依赖关系，形成DAG图，这时候并没有发生真正的计算，只是记录转换的轨迹；\n",
    "*  执行到,collect()是一个行动类型的操作，触发真正的计算，开始实际执行从words到RDD的转换操作，并把结果持久化到内存中，最后计算出RDD中包含的元素个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkContext作为Spark的程序入口\n",
    "from pyspark.context import SparkContext, SparkConf\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.191.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "# 输出Spark基本信息\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/home/gavin/Machine/PySpark/Spark基础/data/text.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取词信息\n",
    "path = os.path.join(os.getcwd(), \"data/text.txt\")\n",
    "words = sc.textFile(name=path, minPartitions=5)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Prior versions of the MovieLens dataset included either pre-computed cross-',\n",
       " 'folds or scripts to perform this computation. We no longer bundle either of',\n",
       " 'these features with the dataset, since most modern toolkits provide this as',\n",
       " ' a built-in feature. If you wish to learn about standard approaches to cross',\n",
       " '-fold computation in the context of recommender systems evaluation, see for',\n",
       " ' tools, documentation, and open-source code examples.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建整体流程(只构建，不计算)\n",
    "RDD = words.flatMap(lambda line:line.split(\" \")).map(lambda x:(x, 1)).reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 3),\n",
       " ('to', 3),\n",
       " ('no', 1),\n",
       " ('', 2),\n",
       " ('learn', 1),\n",
       " ('computation', 1),\n",
       " ('see', 1),\n",
       " ('code', 1),\n",
       " ('examples.', 1),\n",
       " ('versions', 1),\n",
       " ('features', 1),\n",
       " ('most', 1),\n",
       " ('you', 1),\n",
       " ('standard', 1),\n",
       " ('-fold', 1),\n",
       " ('documentation,', 1),\n",
       " ('and', 1),\n",
       " ('the', 3),\n",
       " ('included', 1),\n",
       " ('pre-computed', 1),\n",
       " ('scripts', 1),\n",
       " ('this', 2),\n",
       " ('We', 1),\n",
       " ('longer', 1),\n",
       " ('bundle', 1),\n",
       " ('these', 1),\n",
       " ('If', 1),\n",
       " ('approaches', 1),\n",
       " ('in', 1),\n",
       " ('for', 1),\n",
       " ('folds', 1),\n",
       " ('or', 1),\n",
       " ('perform', 1),\n",
       " ('dataset,', 1),\n",
       " ('since', 1),\n",
       " ('toolkits', 1),\n",
       " ('as', 1),\n",
       " ('wish', 1),\n",
       " ('about', 1),\n",
       " ('recommender', 1),\n",
       " ('tools,', 1),\n",
       " ('Prior', 1),\n",
       " ('MovieLens', 1),\n",
       " ('dataset', 1),\n",
       " ('either', 2),\n",
       " ('cross-', 1),\n",
       " ('computation.', 1),\n",
       " ('with', 1),\n",
       " ('modern', 1),\n",
       " ('provide', 1),\n",
       " ('a', 1),\n",
       " ('built-in', 1),\n",
       " ('feature.', 1),\n",
       " ('cross', 1),\n",
       " ('context', 1),\n",
       " ('systems', 1),\n",
       " ('evaluation,', 1),\n",
       " ('open-source', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 3),\n",
       " ('to', 3),\n",
       " ('no', 1),\n",
       " ('', 2),\n",
       " ('learn', 1),\n",
       " ('computation', 1),\n",
       " ('see', 1),\n",
       " ('code', 1),\n",
       " ('examples.', 1),\n",
       " ('versions', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#　聚合操作，也即整合所有的流程得到输出结果\n",
    "RDD.collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分布式显示与计算\n",
    "data = [1, 2, 3, 4, 5] \n",
    "# 可以指定计算的分区，也即指定计算的子任务数量(partion)\n",
    "distData = sc.parallelize(data, 5)\n",
    "distData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distData = sc.parallelize(data, 2)\n",
    "# 聚合元素, 相当于聚合操作(累加元素值)\n",
    "distData.reduce(lambda a, b:a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD的常用操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Transformation算子\n",
    "**目的**：从一个已经存在的数据集创建一个新的数据集  \n",
    "**说明**：  \n",
    "(1) 对于RDD而言，每一次转换操作都会产生不同的RDD，供给下一个“转换”使用.  \n",
    "(2) 转换得到的RDD是惰性求值的，也就是说，整个转换过程只是记录了转换的轨迹，并不会发生真正的计算，只有遇到行动操作时，才会发生真正的计算，开始从血缘关系源头开始，进行物理的转换操作．  \n",
    "**常用转换操作**：  \n",
    "![](./imgs/transform.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map(fun)\n",
    "rdd1 = sc.parallelize([1, 2, 3, 5, 6, 7, 8], 5)\n",
    "rdd2 = rdd1.map(lambda x: x+1)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 8, 9]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter(fun)\n",
    "rdd3 = rdd2.filter(lambda x:x>5)\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'x', 'd', 'u', 'j', 'l', 'p', 'o'] [['a', 'b', 'x'], ['d', 'u', 'j'], ['l', 'p', 'o']]\n"
     ]
    }
   ],
   "source": [
    "# flatMap操作, 先执行map操作，后执行flatc操作\n",
    "rdd4 = sc.parallelize([\"a b x\", \"d u j\", \"l p o\"], 4)\n",
    "rdd5 = rdd4.flatMap(lambda x: x.split(\" \"))\n",
    "rdd6 = rdd4.map(lambda x:x.split(\" \"))\n",
    "print(rdd5.collect(), rdd6.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 2), ('c', 1), ('b', 3)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# union 求并集\n",
    "rdd1 = sc.parallelize([(\"a\",1),(\"b\",2)])\n",
    "rdd2 = sc.parallelize([(\"c\",1),(\"b\",3)])\n",
    "rdd3 = rdd1.union(rdd2)\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', <pyspark.resultiterable.ResultIterable at 0x7f531c1b0340>),\n",
       " ('b', <pyspark.resultiterable.ResultIterable at 0x7f531c1b0cd0>),\n",
       " ('c', <pyspark.resultiterable.ResultIterable at 0x7f531c394eb0>)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupByKey [(\"b\", [2, 3])]\n",
    "rdd4 = rdd3.groupByKey()\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(rdd4.collect()[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 5), ('c', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey，相同key值的value值相加\n",
    "rdd5 = rdd3.reduceByKey(lambda x, y:x+y)\n",
    "rdd5.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mary', 1), ('a', 3), ('had', 2), ('lamb', 5), ('little', 4)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortByKey\n",
    "temp = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
    "rdd = sc.parallelize(temp, 5)\n",
    "rdd1 = rdd.sortByKey()\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lamb', 5), ('little', 4), ('a', 3), ('had', 2), ('Mary', 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortBy\n",
    "rdd2 = rdd.sortBy(keyfunc=lambda x:x[1], ascending=False)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Action算子\n",
    "**说明**：  \n",
    "行动操作是真正触发计算的地方。Spark程序执行到行动操作时，才会执行真正的计算，从文件中加载数据，完成一次又一次转换操作，最终，完成行动操作得到结果。  \n",
    "**惰性机制**:  \n",
    "所谓的“惰性机制”是指，整个转换过程只是记录了转换的轨迹，并不会发生真正的计算，只有遇到行动操作时，才会触发“从头到尾”的真正的计算。  \n",
    "**常用动作操作**:  \n",
    "![](./imgs/action.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Mary', 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first\n",
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mary', 1), ('had', 2)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take(显示指定个数的元素)\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('little', 4), ('lamb', 5)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.top(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.foreach(lambda x: print(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce累加\n",
    "rdd_re = sc.parallelize([2, 3, 5, 4])\n",
    "rdd_re.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 持久化\n",
    "**持久化的由来**:  \n",
    "在Spark中，RDD采用惰性求值的机制，每次遇到行动操作，都会从头开始执行计算。每次调用行动操作，都会触发一次从头开始的计算。这对于迭代计算而言，代价是很大的，迭代计算经常需要多次重复使用同一组数据,因此将元素多次使用的RDD保存到内存中，可以加快计算速度。  \n",
    "**说明**：  \n",
    "- 可以通过持久化（缓存）机制避免这种重复计算的开销\n",
    "- 可以使用persist()方法对一个RDD标记为持久化\n",
    "- 之所以说“标记为持久化”，是因为出现persist()语句的地方，并不会马上计算生成RDD并把它持久化，而是要等到遇到第一个行动操作触发真正计算以后，才会把计算结果进行持久化\n",
    "- 持久化后的RDD将会被保留在计算节点的内存中被后面的行动操作重复使用  \n",
    "**常用操作**：  \n",
    "![](./imgs/persist.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "name_array = [\"Hadoop\", \"Spark\", \"Hive\"]\n",
    "rdd = sc.parallelize(name_array)\n",
    "# 数据缓存到内存中，在遇见第一个行动操作时执行(可以使用rdd.cache() 保存到内存中)\n",
    "rdd.persist()\n",
    "print(rdd.count())  # 从头到尾执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop,Spark,Hive\n"
     ]
    }
   ],
   "source": [
    "print(\",\".join(rdd.collect()))  # 直接使用缓存在内存中的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[76] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不在使用时,从内存中释放\n",
    "rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分区\n",
    "**分区说明**:  \n",
    "RDD是弹性分布式数据集，通常RDD很大，会被分成很多个分区，分别保存在不同的节点上。  \n",
    "**分区的优点**:  \n",
    "（１）增加并行度  \n",
    "（２）减少通讯开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2], [3], [4], [5]]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分区\n",
    "data = sc.parallelize([1, 2, 3, 4, 5], 5)\n",
    "data.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3, 4], [2, 5]]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重新分区\n",
    "rdd = data.repartition(2)\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 1)], [(2, 1)], [(3, 1)], [(4, 1)], [(5, 1)]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = data.map(lambda x:(x, 1))\n",
    "rdd1.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd1.partitionBy(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(2, 1), (4, 1)], [(1, 1), (3, 1), (5, 1)]]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfPartition:\n",
    "    \n",
    "    def myPartition(self, key):\n",
    "        \"\"\"自定义分区\"\"\"\n",
    "        \n",
    "        print(\"My partition is running!\")\n",
    "        print(\"The key is %d\"%key)\n",
    "        \n",
    "        return key%10\n",
    "    \n",
    "    def main(self):\n",
    "        \"\"\"自定义分区函数\"\"\"\n",
    "#         sc.stop()\n",
    "        print(\"The main funcation is running!\")\n",
    "        conf = SparkConf().setMaster(\"local\").setAppName(\"MyApp\")\n",
    "        sc = SparkContext(conf=conf)\n",
    "        \n",
    "        data = sc.parallelize(range(10), 5)\n",
    "        data.map(lambda x:(x, 1)).partitionBy(10, self.myPartition).map(lambda x:x[0]).saveAsTextFile(\"./data/pp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main funcation is running!\n"
     ]
    }
   ],
   "source": [
    "sp = SelfPartition()\n",
    "sp.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 实战１：通过Spark实现点击流日志分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网络总访问量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"pv\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "path1 = os.path.join(os.getcwd(), \"data/access.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-94ac03480351>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 网络信息\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path1' is not defined"
     ]
    }
   ],
   "source": [
    "# 网络信息\n",
    "rdd = sc.textFile(path1)\n",
    "rdd.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pv', 90)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每一行数据表示一次网站访问(访问量)\n",
    "rdd1 = rdd.map(lambda x:(\"pv\", 1)).reduceByKey(lambda a, b :a+b)\n",
    "# rdd1.saveAsTextFile(\"data/pv.txt\")\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网站独立用户访问量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"uv\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "path1 = os.path.join(os.getcwd(), \"data/access.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['194.237.142.21',\n",
       " '183.49.46.228',\n",
       " '163.177.71.12',\n",
       " '163.177.71.12',\n",
       " '101.226.68.137']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得到ip地址\n",
    "rdd1 = sc.textFile(path1)\n",
    "rdd2 = rdd1.map(lambda x:x.split(\" \")).map(lambda x:x[0])\n",
    "rdd2.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('uv', 17)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = rdd2.distinct().map(lambda x:(\"uv\", 1))\n",
    "rdd4 = rdd3.reduceByKey(lambda a,b:a+b)\n",
    "rdd4.collect()\n",
    "# rdd4.saveAsTextFile(\"data/uv.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 访问TopN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"TopN\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "path1 = os.path.join(os.getcwd(), \"data/access.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\"-\"', 27),\n",
       " ('\"http://blog.fens.me/vps-ip-dns/\"', 18),\n",
       " ('\"http://blog.fens.me/wp-content/themes/silesia/style.css\"', 7),\n",
       " ('\"http://blog.fens.me/nodejs-socketio-chat/\"', 7),\n",
       " ('\"http://blog.fens.me/nodejs-grunt-intro/\"', 7)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.textFile(path1)\n",
    "rdd2 = rdd1.map(lambda line:line.split(\" \")).filter(lambda x:len(x)>10).map(lambda x:(x[10],1))\n",
    "rdd3 = rdd2.reduceByKey(lambda a, b:a+b).sortBy(lambda x:x[1], ascending=False)\n",
    "rdd4 = rdd3.take(5)\n",
    "rdd4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('\"-\"', 27),\n",
       "  ('\"http://blog.fens.me/vps-ip-dns/\"', 18),\n",
       "  ('\"http://blog.fens.me/wp-content/themes/silesia/style.css\"', 7),\n",
       "  ('\"http://blog.fens.me/nodejs-socketio-chat/\"', 7),\n",
       "  ('\"http://blog.fens.me/nodejs-grunt-intro/\"', 7)],\n",
       " [('\"http://blog.fens.me/nodejs-async/\"', 5),\n",
       "  ('\"http://www.angularjs.cn/A00n\"', 2),\n",
       "  ('\"http://cos.name/category/software/packages/\"', 1),\n",
       "  ('\"http://blog.fens.me/series-nodejs/\"', 1),\n",
       "  ('\"http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=6&cad=rja&ved=0CHIQFjAF&url=http%3A%2F%2Fblog.fens.me%2Fvps-ip-dns%2F&ei=j045UrP5AYX22AXsg4G4DQ&usg=AFQjCNGsJfLMNZnwWXNpTSUl6SOEzfF6tg&sig2=YY1oxEybUL7wx3IrVIMfHA&bvm=bv.52288139,d.b2I\"',\n",
       "   1),\n",
       "  ('\"http://www.google.com/url?sa=t&rct=j&q=nodejs%20%E5%BC%82%E6%AD%A5%E5%B9%BF%E6%92%AD&source=web&cd=1&cad=rja&ved=0CCgQFjAA&url=%68%74%74%70%3a%2f%2f%62%6c%6f%67%2e%66%65%6e%73%2e%6d%65%2f%6e%6f%64%65%6a%73%2d%73%6f%63%6b%65%74%69%6f%2d%63%68%61%74%2f&ei=rko5UrylAefOiAe7_IGQBw&usg=AFQjCNG6YWoZsJ_bSj8kTnMHcH51hYQkAA&bvm=bv.52288139,d.aGc\"',\n",
       "   1),\n",
       "  ('\"http://www.angularjs.cn/\"', 1)]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得到partion的分组情况\n",
    "rdd3.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 终止\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实战2：实现ip地址查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"IPSearch\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.0.1.0|1.0.3.255|16777472|16778239|亚洲|中国|福建|福州||电信|350100|China|CN|119.306239|26.075302',\n",
       " '1.0.8.0|1.0.15.255|16779264|16781311|亚洲|中国|广东|广州||电信|440100|China|CN|113.280637|23.125178']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_id_rdd = sc.textFile(\"data/ip.txt\")\n",
    "city_id_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('16777472', '16778239', '福州', '119.306239', '26.075302'),\n",
       " ('16779264', '16781311', '广州', '113.280637', '23.125178'),\n",
       " ('16785408', '16793599', '广州', '113.280637', '23.125178'),\n",
       " ('16842752', '16843007', '福州', '119.306239', '26.075302'),\n",
       " ('16843264', '16844799', '福州', '119.306239', '26.075302')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 起始地址 截止地址  经度 纬度\n",
    "city_id_rdd = city_id_rdd.map(lambda x:x.split(\"|\")).map(lambda x:(x[2], x[3],x[7], x[13], x[14]))\n",
    "city_id_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('16777472', '16778239', '福州', '119.306239', '26.075302'),\n",
       " ('16779264', '16781311', '广州', '113.280637', '23.125178')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#　创建广播变量，使其不需要复制到每一个数据集当中\n",
    "city_brodcast = sc.broadcast(city_id_rdd.collect())\n",
    "city_brodcast.value[:2]  # 列表形式存取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['125.213.100.123',\n",
       " '117.101.215.133',\n",
       " '117.101.222.68',\n",
       " '115.120.36.118',\n",
       " '123.197.64.247']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获得相对应的ip地址\n",
    "dest_data = sc.textFile(\"data/20090121000132.394251.http.format\").map(lambda x:x.split(\"|\")[1])\n",
    "dest_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ip_transform(ip):\n",
    "    \"\"\"ip地址装换\"\"\"\n",
    "    \n",
    "    ips = ip.split(\".\")#[223,243,0,0] 32位二进制数\n",
    "    ip_num = 0\n",
    "    for i in ips:\n",
    "        ip_num = int(i) | ip_num << 8\n",
    "    return ip_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search(ip_num, broadcast_value):\n",
    "    start = 0\n",
    "    end = len(broadcast_value) - 1\n",
    "    while (start <= end):\n",
    "        mid = int((start + end) / 2)\n",
    "        if ip_num >= int(broadcast_value[mid][0]) and ip_num <= int(broadcast_value[mid][1]):\n",
    "            return mid\n",
    "        if ip_num < int(broadcast_value[mid][0]):\n",
    "            end = mid\n",
    "        if ip_num > int(broadcast_value[mid][1]):\n",
    "            start = mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(x):\n",
    "    \"\"\"ip地址\"\"\"\n",
    "    \n",
    "    city_brodcast_value = city_brodcast.value\n",
    "    def get_result(ip):\n",
    "        ip_num = ip_transform(ip)\n",
    "        # index表示相对应的ip的位置信息的索引结点\n",
    "        index = binary_search(ip_num, city_brodcast_value)\n",
    "        \n",
    "        # ((经度, 纬度, 地址), 1))\n",
    "        return ((city_brodcast_value[index][2], city_brodcast_value[index][3], city_brodcast_value[index][4]), 1)\n",
    "    \n",
    "    x = map(tuple, [get_result(ip) for ip in x])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照Partitiion传入数据块进行运算\n",
    "dest_rdd = dest_data.mapPartitions(lambda x:get_pos(x))\n",
    "result_rdd = dest_rdd.reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('西安', '108.948024', '34.263161'), 1824),\n",
       " (('重庆', '107.7601', '29.32548'), 85),\n",
       " (('重庆', '106.504962', '29.533155'), 400),\n",
       " (('重庆', '106.27633', '29.97227'), 36),\n",
       " (('重庆', '107.39007', '29.70292'), 47),\n",
       " (('重庆', '106.56347', '29.52311'), 3),\n",
       " (('重庆', '107.08166', '29.85359'), 29),\n",
       " (('北京', '116.405285', '39.904989'), 1535),\n",
       " (('石家庄', '114.502461', '38.045474'), 383),\n",
       " (('重庆', '106.51107', '29.50197'), 91),\n",
       " (('昆明', '102.712251', '25.040609'), 126),\n",
       " (('重庆', '106.57434', '29.60658'), 177)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 统计相同地址经纬度下的sun\n",
    "result_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ubuntu18.04安装spark（伪分布式）](https://blog.csdn.net/weixin_42001089/article/details/82346367#commentBox)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
